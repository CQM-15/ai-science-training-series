{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71e7530d-052b-479a-9a9b-4e9050c43c76",
   "metadata": {},
   "source": [
    "# Argone Leadership Computing Facility Artificial Intelligence Training Program\n",
    "## Cesar Francisco Quinones-Martinez (cquinones24)\n",
    "Session 1: Intro to Artificial Intelligence on Supercomputers, 2024-02-09"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d80670-a03d-40c5-9dc8-4d60e6c335df",
   "metadata": {},
   "source": [
    "In this Notebook, I apply the information given by Dr. Huihuo Zheng about linear regression to work on the Homework assignment. I first pass the iterative version of linear regression to verify that it functions in this copy. Afterwards, I will include the changes that were asked for in the Homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5be4abb8-c5b2-430a-820c-f9c9f63ca5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'SalePrice', 'GrLivArea'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Importing the relevant libraries and necessary real estate data:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipydis\n",
    "import time\n",
    "\n",
    "! [ -e ./slimmed_realestate_data.csv ] || wget https://raw.githubusercontent.com/argonne-lcf/ai-science-training-series/main/01_intro_AI_on_Supercomputer/slimmed_realestate_data.csv\n",
    "data = pd.read_csv('slimmed_realestate_data.csv')\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e843e7e",
   "metadata": {},
   "source": [
    "We define the data and linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b21d573-d62b-41ba-9383-fa9a70f14742",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('slimmed_realestate_data.csv')\n",
    "data_x = data['GrLivArea'].to_numpy()\n",
    "data_y = data['SalePrice'].to_numpy()\n",
    "\n",
    "# Linear Regression by theory:\n",
    "\n",
    "n = len(data)\n",
    "sum_xy = np.sum(data_x*data_y)\n",
    "sum_x = np.sum(data_x)\n",
    "sum_y = np.sum(data_y)\n",
    "sum_x2 = np.sum(data_x*data_x)\n",
    "denominator = n * sum_x2 - sum_x * sum_x\n",
    "\n",
    "m_calc = (n * sum_xy - sum_x * sum_y) / denominator\n",
    "b_calc = (sum_y * sum_x2 - sum_x * sum_xy) / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d524bc",
   "metadata": {},
   "source": [
    "Defining the necessary functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923f61ec-5fec-4a8d-bb96-43d5bc43938f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x,m,b):\n",
    "    return m * x + b\n",
    "\n",
    "def loss(x,y,m,b):\n",
    "    y_predicted = model(x,m,b)\n",
    "    return np.power(y - y_predicted,2)\n",
    "\n",
    "def updated_m(x,y,m,b,learning_rate):\n",
    "    dL_dm = - 2 * x * (y - model(x,m,b))\n",
    "    dL_dm = np.mean(dL_dm)\n",
    "    return m - learning_rate * dL_dm\n",
    "\n",
    "def updated_b(x,y,m,b,learning_rate):\n",
    "    dL_db = - 2 * (y - model(x,m,b))\n",
    "    dL_db = np.mean(dL_db)\n",
    "    return b - learning_rate * dL_db\n",
    "\n",
    "def plot_data(x,y,m,b,plt = plt):\n",
    "    # plot our data points with 'bo' = blue circles\n",
    "    plt.plot(x,y,'bo')\n",
    "    # create the line based on our linear fit\n",
    "    # first we need to make x points\n",
    "    # the 'arange' function generates points between two limits (min,max)\n",
    "    linear_x = np.arange(x.min(),x.max())\n",
    "    # now we use our fit parameters to calculate the y points based on our x points\n",
    "    linear_y = linear_x * m + b\n",
    "    # plot the linear points using 'r-' = red line\n",
    "    plt.plot(linear_x,linear_y,'r-',label='fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9701e151-466d-412e-8985-a3ee539780e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Using the Stochastic Gradient Descent (SGD) Method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144b08b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial guess:\n",
    "m = 5.\n",
    "b = 1000.\n",
    "batch_scale = 1\n",
    "learning_m_scale = 1\n",
    "learning_b_scale = 1\n",
    "# Learning rates:\n",
    "learning_rate_m = 1e-7*learning_m_scale\n",
    "learning_rate_b = 1e-1*learning_b_scale\n",
    "\n",
    "loss_history = []\n",
    "batch_size = 64*batch_scale\n",
    "\n",
    "data_batch = data.sample(batch_size)\n",
    "data_x = data_batch['GrLivArea'].to_numpy()\n",
    "data_y = data_batch['SalePrice'].to_numpy()\n",
    "\n",
    "if batch size > n:\n",
    "    batch_size = len(data)\n",
    "\n",
    "loop_N = 30*len(data)//batch_size\n",
    "\n",
    "for i in range(loop_N):\n",
    "    # update our slope and intercept based on the current values\n",
    "    m = updated_m(data_x,data_y,m,b,learning_rate_m)\n",
    "    b = updated_b(data_x,data_y,m,b,learning_rate_b)\n",
    "    \n",
    "    loss_value = np.mean(loss(data_x,data_y,m,b))\n",
    "    loss_history.append(loss_value)\n",
    "    print('[%03d]  dy_i = %.2f * x + %.2f     previously calculated: y_i = %.2f * x + %.2f    loss: %f' % (i,m,b,m_calc,b_calc,loss_value))\n",
    "    \n",
    "    plt.close('all')\n",
    "    \n",
    "    fig,ax = plt.subplots(1,2,figsize=(18,6),dpi=80)\n",
    "    # lot our usual output\n",
    "    plot_data(data_x,data_y,m,b,ax[0])\n",
    "    # here we also plot the calculated linear fit for comparison\n",
    "    line_x = np.arange(data_x.min(),data_x.max())\n",
    "    line_y = line_x * m_calc + b_calc\n",
    "    ax[0].plot(line_x,line_y,'b-',label='calculated')\n",
    "    # add a legend to the plot and x/y labels\n",
    "    ax[0].legend()\n",
    "    ax[0].set_xlabel('square footage')\n",
    "    ax[0].set_ylabel('sale price')\n",
    "    \n",
    "    # plot the loss \n",
    "    loss_x = np.arange(0,len(loss_history))\n",
    "    loss_y = np.asarray(loss_history)\n",
    "    ax[1].plot(loss_x,loss_y, 'o-')\n",
    "    ax[1].set_yscale('log')\n",
    "    ax[1].set_xlabel('loop step')\n",
    "    ax[1].set_ylabel('loss')\n",
    "    plt.show()\n",
    "    # gives us time to see the plot\n",
    "    time.sleep(2.5)\n",
    "    # clears the plot when the next plot is ready to show.\n",
    "    ipydis.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed5a0fd",
   "metadata": {},
   "source": [
    "Applying the data_batch = data.sample(batch_size) line takes only the selected number of data points from the data to work on. With the batch_scale variable we increase the batch size by multiplying it by a number we define. If the scaling variable goes above 8 it outputs a number bigger than the total number of data points, which can affect the total number of loops, where the for loop eliminates that possible issue.\n",
    "\n",
    "Depending on the batch size, the fit we are obtaining with SGD can be more innacurate when compared to the Linear Regression fit as it may be weighted to less data that can be more concentrated to on one side. However this shows that with correct bath sizes you can obtain similar trends in your data without having to model for all datapoints, saving compute time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5f5525",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "batch_scale = 1\n",
    "learning_m_scale = 1\n",
    "learning_b_scale = 1\n",
    "'''\n",
    "# Allow to modify the behavior of the learning rates.\n",
    "# Data about 551 points, such that values above 8 for batch_scale gives all points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac1908d",
   "metadata": {},
   "source": [
    "For equal scale values, SGD method diverges from the fit at a value of 8. At learning_b_scale >= 14 the SGD fit diverges when using all data points, boucing around the fit and the loss increasing at it kept bouncing. Similar behavior occurs with learning_m_scale >= 5 where the fit completely diverges."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience/conda-2023-01-10",
   "language": "python",
   "name": "conda-2023-01-10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
